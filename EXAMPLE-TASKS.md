# Task Sequence

## Decision Memory

Unified tech stack and architectural decisions for chat-analyzer:

1. Core Dependencies: Use github.com/mattn/go-sqlite3 for SQLite, github.com/joho/godotenv for env vars, github.com/sashabaranov/go-openai for LLM/Embedding APIs, github.com/sirupsen/logrus for structured logging, github.com/urfave/cli/v2 for CLI, github.com/olekukonko/tablewriter for markdown tables. Omit redis/go-redis/v9 and ollama/api unless explicitly required later; prioritize simplicity (KISS). Use github.com/google/uuid for ID generation where UUIDs are needed for referential integrity.

2. Project Structure: Standardized layout: cmd/, internal/, pkg/, scripts/, docs/, testdata/. All core logic under internal/; pkg/ for reusable utilities; cmd/ for entry points; scripts/ for setup/test; docs/ for documentation. Internal packages: config, logger, utils, db, types, parser, embedding, clustering, summarization, usage, pipeline. Pkg packages: config (for external config access), docs (for documentation generation).

3. Configuration: Single config package under internal/config that loads env vars with defaults: LLM_API_KEY, EMB_API_KEY, LLM_URL, EMB_URL, CONTEXT_LIMIT, CLUSTERING_THRESHOLD, USER_TEXT_MAX_LINES, AGENT_TEXT_MAX_LINES, AGENT_COMMAND_MAX_LINES, FORCE_REGENERATE, VERBOSE. Use godotenv for .env loading. All components access config via shared instance (SOLID: Dependency Inversion). Add DB_PATH, PROMPT_TEMPLATE_PATH, RETRY_MAX_ATTEMPTS, RETRY_BASE_DELAY_MS, LOG_LEVEL, DEBUG as needed. Validate required fields using github.com/go-playground/validator/v10.

4. Logging: Internal logger package using logrus with JSON formatter, levels (INFO, WARN, ERROR), and optional file output. All modules inject logger via interface for testability (SOLID: Dependency Inversion). Add context fields for file, line, operation.

5. Database: SQLite with WAL mode, pragma settings for performance. Schema defined in internal/db/schema.sql with normalized tables: chats, messages, tasks, usage_events, clusters, stats, correlation. Indexes on timestamp and start_line. Migration via full rebuild. All SQL uses prepared statements. Database manager in internal/db with connection pooling, transaction support, and schema validation. Use UUIDs for primary keys where referential integrity is critical.
...

---

## Tasks

## Task 1: Initialize Go Project Structure and Core Dependencies

Create a new Go module named `chat-analyzer` with a standardized directory layout: `cmd/`, `internal/`, `pkg/`, `scripts/`, `docs/`, and `testdata/`. Initialize `go.mod` with Go 1.21+ and add required dependencies: `github.com/mattn/go-sqlite3` for SQLite, `github.com/joho/godotenv` for environment variables, `github.com/sashabaranov/go-openai` for LLM integration, `github.com/sirupsen/logrus` for structured logging, `github.com/urfave/cli/v2` for CLI argument parsing, `github.com/olekukonko/tablewriter` for markdown table generation, `github.com/google/uuid` for UUID generation, and `github.com/go-playground/validator/v10` for config validation. Define a `config` package under `internal/` that loads environment variables with defaults: `LLM_API_KEY`, `EMB_API_KEY`, `LLM_URL`, `EMB_URL`, `CONTEXT_LIMIT`, `CLUSTERING_THRESHOLD`, `USER_TEXT_MAX_LINES`, `AGENT_TEXT_MAX_LINES`, `AGENT_COMMAND_MAX_LINES`, `FORCE_REGENERATE`, `VERBOSE`, `DB_PATH`, `PROMPT_TEMPLATE_PATH`, `RETRY_MAX_ATTEMPTS`, `RETRY_BASE_DELAY_MS`, `LOG_LEVEL`, `DEBUG`. Implement a `logger` package under `internal/` that provides structured logging with levels (INFO, WARN, ERROR) and supports output to stdout or file using logrus with JSON formatter if `LOG_FORMAT=json`, otherwise text. Create a `utils` package under `internal/` with helper functions: `FileExists(path string) bool`, `ReadFile(path string) ([]byte, error)`, `WriteFile(path string, data []byte) error`, `GetFileSize(path string) (int64, error)`, `GetLineCount(path string) (int, error)`, `NormalizeWhitespace(s string) string`, `TruncateLines(s string, maxLines int) string`, `EnsureDir(path string) error`. Define a `types` package under `internal/` with core structs: `Chat { ID string, StartLine int, EndLine int, Timestamp time.Time, Title string, SourceFile string }`, `Message { ID string, ChatID string, StartLine int, EndLine int, Timestamp time.Time, Role string, Content string, ToolType string, ToolName string, ToolInput string, ToolOutput string }`, `Task { ID string, MessageID string, UserSummary string, AgentSummary string, Embedding []byte, ClusterID string }`, `UsageEvent { ID string, Timestamp time.Time, Tokens int, RequestType string, CacheRead bool, ChatID string }`, `Cluster { ID string, Threshold float64, Tasks []string, Summary string, Name string, Description string }`, `Stats { ID string, Metric string, Value string, Date time.Time }`, `DailyStat { Date time.Time, MinMessages int, MaxMessages int, AvgMessages float64 }`, `ToolStat { ToolType string, Count int }`, `CorrelationStats { TotalRequests int, TotalTokens int, UnmatchedRequestsPercent float64, UnmatchedTokensPercent float64, TokenToCharRatio float64 }`. Implement a `database` package under `internal/` that initializes SQLite connection with WAL mode and pragma settings for performance, creates tables with schema: `chats (id TEXT PRIMARY KEY, start_line INTEGER, end_line INTEGER, timestamp DATETIME, title TEXT, source_file TEXT)`, `messages (id TEXT PRIMARY KEY, chat_id TEXT, start_line INTEGER, end_line INTEGER, timestamp DATETIME, role TEXT, content TEXT, tool_type TEXT, tool_name TEXT, tool_input TEXT, tool_output TEXT, FOREIGN KEY(chat_id) REFERENCES chats(id))`, `tasks (id TEXT PRIMARY KEY, message_id TEXT, user_summary TEXT, agent_summary TEXT, embedding BLOB, cluster_id TEXT, FOREIGN KEY(message_id) REFERENCES messages(id))`, `usage_events (id TEXT PRIMARY KEY, timestamp DATETIME, tokens INTEGER, request_type TEXT, cache_read BOOLEAN, chat_id TEXT, FOREIGN KEY(chat_id) REFERENCES chats(id))`, `clusters (id TEXT PRIMARY KEY, threshold FLOAT, tasks TEXT, summary TEXT, name TEXT, description TEXT)`, `stats (id TEXT PRIMARY KEY, metric TEXT, value TEXT, date DATE)`, `correlation (id TEXT PRIMARY KEY, chat_id TEXT, usage_id TEXT, token_count INTEGER, char_count INTEGER, matched BOOLEAN, FOREIGN KEY(chat_id) REFERENCES chats(id), FOREIGN KEY(usage_id) REFERENCES usage_events(id))`. Ensure all tables have indexes on `timestamp` and `start_line` for efficient querying. Write unit tests for each package using Go’s built-in testing framework. Expected outcome: A fully initialized Go project with core packages, types, and database schema ready for parsing and analysis.

## Task 2: Implement Markdown Chat Parser with Validation and Incremental Processing

Create a `parser` package under `internal/` that implements a state machine to parse markdown chat logs. The parser must identify chat boundaries using lines starting with `# ` followed by a valid RFC3339 timestamp, extract user/agent messages delimited by `_**` lines, and handle tool blocks with `data-tool-type` and `data-tool-name` attributes. Implement a `ParseFile(path string, startChat int, maxChats int, maxLines int) ([]Chat, []Message, error)` function that reads the file line by line, tracks line numbers, and builds `Chat` and `Message` structs. Validate each message by comparing its start/end line numbers against the original file content to ensure structural fidelity. If a message is malformed, log an error with line context and skip to the next valid chat. Implement incremental parsing: if a database already exists for the input file (replace `.md` with `.db`), query the last processed chat’s timestamp and line number, then resume parsing from the next line. Add a `ValidateContent(db *sql.DB, chatID string, content string, startLine int) error` function that checks if the parsed content matches the original file at the specified line range. Implement progress reporting: every 100 messages, log “Processed X messages from Y chats” to stdout if `VERBOSE` is set. Write test cases in `parser/parser_test.go` using `testdata/EXAMPLE.md` to verify chat/message extraction, line number tracking, tool block parsing, and incremental resume. Expected outcome: A robust parser that transforms markdown logs into structured SQLite records while preserving message boundaries, validating content, and supporting incremental updates.

## Task 3: Generate Parsing Statistics and Daily Aggregates

Extend the `parser` package to compute and store parsing statistics. After parsing, implement a `GenerateStats(db *sql.DB) (Stats, error)` function that queries the database to calculate: total chats, total messages, average messages per chat, total tool calls, tool type distribution (sorted descending), tool group stats (grouped by tool type prefix), and daily activity (per day: min, max, avg messages). Store these stats in a new `stats` table with schema: `id TEXT PRIMARY KEY, metric TEXT, value TEXT, date DATE`. Implement a `RenderStatsMarkdown(stats Stats) string` function that formats the stats into a markdown table with columns: Metric, Value, Date (if daily). For daily stats, transpose the table so dates become rows and metrics become columns. Add a `GetDailyStats(db *sql.DB) []DailyStat` function that returns daily aggregates. Implement a `GetToolStats(db *sql.DB) []ToolStat` function that returns tool usage sorted by count descending. Write unit tests to verify stats calculation matches `testdata/EXAMPLE.md` expectations. Integrate stats generation into the main parsing flow: after inserting chats/messages, call `GenerateStats` and write the markdown output to a temporary buffer. Expected outcome: Comprehensive parsing statistics stored in SQLite and rendered as markdown tables, including daily aggregates and tool usage trends, ready for inclusion in reports.

## Task 4: Implement Usage Event Parser and Correlation Engine

Create a `usage` package under `internal/` that parses CSV usage events from `cursor.com/dashboard`. Implement a `ParseCSV(path string) ([]UsageEvent, error)` function that reads the CSV, validates required columns (timestamp, tokens, request_type, cache_read), and converts timestamps to `time.Time`. Store events in the `usage_events` table. Implement a `CorrelateChatsUsage(chatsDB, usageDB *sql.DB) (CorrelationStats, error)` function that matches user messages followed by agent replies to usage events within a 10-minute window. Use bidirectional matching: for each user message, find the closest usage event after it; for each agent reply, find the closest usage event before it. Exclude `cache_read` tokens and tool call content from correlation. Compute correlation metrics: total requests, total tokens, unmatched requests percentage, unmatched tokens percentage, and token-to-character ratio (agent output tokens vs agent message text length). Store correlation results in a `correlation` table with schema: `id TEXT PRIMARY KEY, chat_id TEXT, usage_id TEXT, token_count INTEGER, char_count INTEGER, matched BOOLEAN`. Implement a `RenderCorrelationMarkdown(stats CorrelationStats) string` function that formats results into a transposed markdown table with rows: Time Interval (1-7 min), Calculation Variant (Average, Overall), and columns: Total Requests, Total Tokens, Matched %, Unmatched %. Write tests using `testdata/EXAMPLE.csv` and `testdata/EXAMPLE.md` to verify correlation accuracy. Expected outcome: A usage parser and correlation engine that links chat interactions with API events, computes efficiency metrics, and outputs transposed tables for trend analysis.

## Task 5: Compute Semantic Embeddings and Store in Database

Create an `embedding` package under `internal/` that generates semantic embeddings for chat tasks. Implement a `TaskEmbedder` struct with methods: `ExtractTasks(db *sql.DB) ([]Task, error)` to fetch all tasks with user/agent summaries, `GenerateEmbeddings(tasks []Task) ([]Task, error)` to call the OpenAI embedding API (using `EMB_API_KEY` and `EMB_URL`), and `StoreEmbeddings(db *sql.DB, tasks []Task) error` to save embeddings as compressed base64 strings in the `tasks` table. Use the `text-embedding-3-small` model by default. Implement exponential backoff with jitter for API retries (max 5 attempts, initial delay 1s, multiplier 2). Add a `GetEmbedding(task Task) ([]float32, error)` function that decompresses and decodes the stored embedding. Implement a `ComputeDistanceMatrix(tasks []Task) ([][]float64, error)` function that computes pairwise cosine distances between all task embeddings. Cache the distance matrix in memory to avoid recomputation. Write unit tests to verify embedding generation, storage, and distance calculation using mock API responses. Expected outcome: A robust embedding pipeline that generates, stores, and retrieves semantic embeddings for tasks, with resilient API handling and distance matrix caching for clustering.

## Task 6: Implement Hierarchical Clustering with Adaptive Thresholds

Create a `clustering` package under `internal/` that groups tasks into coherent clusters. Implement a `Clusterer` struct with methods: `Cluster(tasks []Task, thresholds []float64) ([]Cluster, error)` to apply hierarchical clustering using DBSCAN with adaptive `eps` and `min_samples=1`. For each threshold, compute clusters by recursively splitting large clusters: if a cluster has >10 tasks, split it using DBSCAN with `eps` tuned via binary search on the distance matrix. Store cluster assignments in the `clusters` table with `tasks` as a comma-separated list of task IDs. Implement a `GetClusterStats(db *sql.DB, clusterID string) ClusterStats` function that computes min/avg/max summary lengths per group. Implement a `RenderClusterTable(clusters []Cluster) string` function that formats clusters into a markdown table with columns: Group, Threshold, Tasks Count, Summary Length (Min/Avg/Max). Add progress logging: every 100 tasks processed, log “Clustering progress: X%” to stdout. Write tests using `testdata/EXAMPLE.db` to verify clustering accuracy and progress reporting. Expected outcome: A clustering engine that generates non-overlapping task groups using adaptive thresholds, stores cluster metadata, and outputs summary statistics for analysis.

## Task 7: Generate Task Summaries Using LLM with Structured Prompts

Create a `summarizer` package under `internal/` that generates concise summaries for each cluster. Implement a `Summarizer` struct with methods: `GenerateGroupSummaries(db *sql.DB, clusters []Cluster) ([]Cluster, error)` to fetch all tasks in each cluster, order them by user message timestamp, and send a structured prompt to the LLM. The prompt must be formatted as: “You are an AI assistant that summarizes user-agent interactions. For each group, generate two summaries: one for user requests (imperative voice, max lines configurable) and one for agent actions (tools, files, commands, overall plan, max lines configurable). Output raw markdown directly—no parsing or response section extraction. Content must not be embedded in system prompt; it goes in user message.” The user message should contain: “# {group number}. {5-7 word title} ({first message timestamp})\n**User:** {user summary}\n**Agent:** {agent summary}”. Use `LLM_API_KEY` and `LLM_URL` for API calls. Implement retry logic with exponential backoff. Store the generated summary in the `clusters` table. Implement a `RenderSummariesMarkdown(clusters []Cluster) string` function that formats summaries into a markdown document with a table of contents (TOC) listing group titles. Write tests to verify summary generation and markdown formatting. Expected outcome: A summarizer that produces structured, concise group summaries using LLM, stores them in the database, and renders them with a TOC for easy navigation.

## Task 8: Orchestrate End-to-End Analysis Pipeline

Create a `cmd/chat-analyzer/main.go` that serves as the unified entry point. Implement a `main()` function that: 1) Loads environment variables from `.env` if present. 2) Accepts command-line flags: `-md <file>` (default `EXAMPLE.md`), `-csv <file>` (optional), `-force` (regenerate cached outputs), `-verbose` (enable progress logging). 3) Initializes the database (create if not exists, rebuild if `-force`). 4) Parses the markdown file using the `parser` package. 5) If a CSV is provided, parses usage events and correlates with chats. 6) Extracts tasks and generates embeddings. 7) Clusters tasks using adaptive thresholds. 8) Generates group summaries. 9) Renders a consolidated report to `<md name>-REPORT.md` with sections: Parsing Stats, Usage Correlation (if CSV provided), Task Clustering, Task Summaries. 10) Outputs progress to stdout: file size, line count, embedding progress per 100 tasks, clustering progress. Implement real-time output streaming to prevent hangs. Write a `test.py` script that invokes `main.go` with `EXAMPLE.md` and `EXAMPLE.csv`, verifies report structure, and cleans up test files. Expected outcome: A single executable that orchestrates the full analysis pipeline, produces a standardized report, and supports incremental updates and forced regeneration.

## Task 9: Implement Configuration and Documentation

Update the `config` package to support runtime customization via environment variables. Define defaults for all parameters: `CONTEXT_LIMIT=4096`, `CLUSTERING_THRESHOLD=0.45`, `USER_TEXT_MAX_LINES=20`, `AGENT_TEXT_MAX_LINES=1`, `AGENT_COMMAND_MAX_LINES=3`, `FORCE_REGENERATE=false`, `VERBOSE=false`, `DB_PATH=./chat-analyzer.db`, `PROMPT_TEMPLATE_PATH=./templates/summarize.tmpl`, `RETRY_MAX_ATTEMPTS=5`, `RETRY_BASE_DELAY_MS=1000`, `LOG_LEVEL=info`, `DEBUG=false`. Document all environment variables in `docs/ENVIRONMENT_VARIABLES.md` with descriptions and examples. Update `README.md` to include: project purpose, installation instructions (Go 1.21+, `go mod tidy`), usage examples (`go run cmd/chat-analyzer/main.go -md input.md -csv usage.csv`), script descriptions, and a section on environment variables. Create `AGENTS.md` with atomic, workflow-ordered requirements: “Chat Log Processing: Transform unstructured markdown into structured records...”, “Usage Analytics: Link chats with API events...”, etc. Remove low-level details like filenames or versions. Write a `scripts/setup.sh` that installs dependencies and sets up the environment. Expected outcome: Comprehensive documentation and configuration that enables reproducibility, customization, and onboarding for new users.

## Task 10: Validate and Optimize Full Pipeline

Run the full pipeline on `testdata/cursor-chats.md` and `testdata/usage-events-2025-11-02.csv` using `cmd/chat-analyzer/main.go`. Verify the output report: `cursor-chats-REPORT.md` contains all sections, stats are accurate, clustering groups are meaningful, and summaries are concise. Profile performance: ensure embedding and clustering reuse distance matrices, avoid redundant LLM calls, and handle large files without memory leaks. Optimize: implement streaming for large files, add context size checks via LLM API, and apply aggressive deduplication only on overflow. Fix any bugs: ensure all SQL queries order by `timestamp` then `start_line`, all line numbers are tracked, and all environment variables are used. Update tests to reflect final behavior. Run `go test ./...` to ensure all tests pass. Expected outcome: A fully validated, optimized pipeline that processes real-world data efficiently, produces accurate reports, and adheres to all specifications.
